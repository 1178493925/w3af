"""
crawler.py

Copyright 2018 Andres Riancho

This file is part of w3af, http://w3af.org/ .

w3af is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation version 2 of the License.

w3af is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with w3af; if not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA

"""
import time

import w3af.core.controllers.output_manager as om

from w3af.core.controllers.chrome.pool import ChromePool
from w3af.core.data.fuzzer.utils import rand_alnum


class ChromeCrawler(object):
    """
    Use Google Chrome to crawl a site.

    The basic steps are:
        * Get an InstrumentedChrome instance from the chrome pool
        * Load a URL
        * Receive the HTTP requests generated during loading
        * Send the HTTP requests to the caller
    """

    RENDERED_VS_RAW_RATIO = 0.1

    def __init__(self, uri_opener, max_instances=None, web_spider=None):
        """

        :param uri_opener: The uri opener required by the InstrumentedChrome
                           instances to open URLs via the HTTP proxy daemon.

        :param max_instances: Max number of Chrome processes to spawn. Use None
                              to let the pool decide what is the max.

        :param web_spider: A web_spider instance which is used (if provided) to
                           parse the DOM rendered by Chrome instances.
        """
        self._uri_opener = uri_opener
        self._web_spider = web_spider
        self._pool = ChromePool(self._uri_opener, max_instances=max_instances)

    def crawl(self, url, http_traffic_queue):
        """
        :param url: The URL to crawl
        :param http_traffic_queue: Queue.Queue() where HTTP requests and responses
                                   generated by the browser are sent

        :return: True if the crawling process completed successfully, otherwise
                 exceptions are raised.
        """
        debugging_id = rand_alnum(8)

        args = (url, debugging_id)
        msg = 'Starting chrome crawler for %s (did: %s)'

        om.out.debug(msg % args)

        crawler_http_traffic_queue = CrawlerHTTPTrafficQueue(http_traffic_queue,
                                                             debugging_id=debugging_id)

        try:
            chrome = self._pool.get(http_traffic_queue=crawler_http_traffic_queue)
        except Exception, e:
            args = (e, debugging_id)
            msg = 'Failed to get a chrome instance: "%s" (did: %s)'
            om.out.debug(msg % args)

            raise ChromeCrawlerException('Failed to get a chrome instance: "%s"' % e)

        args = (chrome, url, debugging_id)
        om.out.debug('Using %s to load %s (did: %s)' % args)

        chrome.set_debugging_id(debugging_id)
        start = time.time()

        try:
            chrome.load_url(url)
        except Exception, e:
            args = (url, chrome, e, debugging_id)
            msg = 'Failed to load %s using %s: "%s" (did: %s)'
            om.out.debug(msg % args)

            # Since we got an error we remove this chrome instance from the pool
            # it might be in an error state
            self._pool.remove(chrome)

            args = (url, chrome, e)
            raise ChromeCrawlerException('Failed to load %s using %s: "%s"' % args)

        try:
            successfully_loaded = chrome.wait_for_load()
        except Exception, e:
            #
            # Note: Even if we get here, the InstrumentedChrome might have sent
            # a few HTTP requests. Those HTTP requests are immediately sent to
            # the output queue.
            #
            args = (url, chrome, e, debugging_id)
            msg = ('Exception raised while waiting for page load of %s '
                   'using %s: "%s" (did: %s)')
            om.out.debug(msg % args)

            # Since we got an error we remove this chrome instance from the pool
            # it might be in an error state
            self._pool.remove(chrome)

            args = (url, chrome, e)
            msg = ('Exception raised while waiting for page load of %s '
                   'using %s: "%s"')
            raise ChromeCrawlerException(msg % args)

        if not successfully_loaded:
            #
            # I need to pause the chrome browser so it doesn't continue loading
            #
            spent = time.time() - start
            msg = 'Chrome did not successfully load %s in %.2f seconds (did: %s)'
            args = (url, spent, debugging_id)
            om.out.debug(msg % args)

        #
        # Even if the page has successfully loaded (which is a very subjective
        # term) we click on the stop button to prevent any further requests,
        # changes, etc.
        #
        try:
            chrome.stop()
        except Exception, e:
            msg = 'Failed to stop chrome browser %s: "%s" (did: %s)'
            args = (chrome, e, debugging_id)
            om.out.debug(msg % args)

            # Since we got an error we remove this chrome instance from the
            # pool it might be in an error state
            self._pool.remove(chrome)

            raise ChromeCrawlerException('Failed to stop chrome browser')

        #
        # Extract links and forms from the rendered HTML. In some cases the
        # rendered DOM / HTML and then HTML received in the HTTP response are
        # very different, so we send the rendered DOM to the DocumentParser to
        # extract new information
        #
        self._parse_dom(chrome, debugging_id)

        #
        # In order to remove all the DOM from the chrome instance and clear
        # some memory we load the about:blank page
        #
        try:
            chrome.load_about_blank()
        except Exception, e:
            msg = 'Failed to load about:blank page in chrome browser %s: "%s" (did: %s)'
            args = (chrome, e, debugging_id)
            om.out.debug(msg % args)

            # Since we got an error we remove this chrome instance from the
            # pool it might be in an error state
            self._pool.remove(chrome)

            raise ChromeCrawlerException('Failed to load about:blank in chrome browser')

        # Success! Return the chrome instance to the pool
        self._pool.free(chrome)

        spent = time.time() - start
        args = (crawler_http_traffic_queue.count, url, spent, chrome, debugging_id)
        msg = 'Extracted %s new HTTP requests from %s in %.2f seconds using %s (did: %s)'
        om.out.debug(msg % args)

        return True

    def _parse_dom(self, chrome, debugging_id):
        """
        Parses the DOM that is loaded into Chrome using DocumentParser.

        :param chrome: An InstrumentedChrome instance
        :return: None, the new URLs and forms are sent to the chrome instance
                 http_traffic_queue.
        """
        # In some cases (mostly unittests) we don't have a web spider instance,
        # so we can't parse the DOM. Just ignore all this method.
        if self._web_spider is None:
            return

        try:
            dom = chrome.get_dom()
            assert dom is not None, 'DOM was None'
        except Exception, e:
            msg = 'Failed to get the DOM from chrome browser %s: "%s" (did: %s)'
            args = (chrome, e, debugging_id)
            om.out.debug(msg % args)

            # Since we got an error we remove this chrome instance from the
            # pool, it might be in an error state
            self._pool.remove(chrome)

            raise ChromeCrawlerException('Failed to get the DOM from chrome browser')

        first_http_response = chrome.get_first_response()
        if first_http_response is None:
            msg = 'The %s browser first HTTP response is None (did: %s)'
            args = (chrome, debugging_id)
            om.out.debug(msg % args)
            return

        if not self._should_parse_dom(dom, first_http_response.get_body()):
            msg = 'Decided not to parse the DOM (did: %s)'
            args = (debugging_id,)
            om.out.debug(msg % args)
            return

        first_http_request = chrome.get_first_request()

        dom_http_response = first_http_response.copy()
        dom_http_response.set_body(dom)

        web_spider = self._web_spider
        web_spider.extract_html_forms(dom_http_response, first_http_request)
        web_spider.extract_links_and_verify(dom_http_response, first_http_request)

    def _should_parse_dom(self, dom, raw_body):
        """
        Call the DocumentParser (which is an expensive process) only if the
        rendered response shows significant changes. Just measure the size,
        any increase / decrease will make it.

        :return: True if we should parse the DOM
        """
        raw_body_len = len(raw_body)
        rendered_len = len(dom)

        if rendered_len > raw_body_len * (1 + self.RENDERED_VS_RAW_RATIO):
            return True

        if rendered_len < raw_body_len * (1 - self.RENDERED_VS_RAW_RATIO):
            return True

        return False

    def terminate(self):
        self._pool.terminate()
        self._uri_opener = None
        self._web_spider = None


class CrawlerHTTPTrafficQueue(object):
    def __init__(self, http_traffic_queue, debugging_id):
        self.http_traffic_queue = http_traffic_queue
        self.debugging_id = debugging_id
        self.count = 0

    def put(self, request_response):
        self.count += 1

        # msg = 'Received HTTP traffic from chrome in output queue. Count is %s (did: %s)'
        # args = (self.count, self.debugging_id)
        # om.out.debug(msg % args)

        return self.http_traffic_queue.put(request_response)


class ChromeCrawlerException(Exception):
    pass
